# RL Maze Navigation: DP vs Monte Carlo

A clean, well-documented implementation comparing **Dynamic Programming** and **Monte Carlo** reinforcement learning algorithms on maze navigation.

**Status**: Complete âœ… | **Python**: 3.8+ | **License**: MIT

## ğŸš€ Quick Start

```bash
# 1. Clone & setup
git clone <repo-url>
cd reinforcement-learning-maze
python -m venv venv && source venv/bin/activate

# 2. Install & run
pip install -r requirements.txt
python task1_dynamic_programming/task1_main.py
python task2_monte_carlo/task2_main.py
python task3_analysis/task3_main.py
```

See **[QUICKSTART.md](QUICKSTART.md)** for detailed setup guide.

## ğŸ“Š Key Results

| Metric | DP | MC | Winner |
|--------|----|----|--------|
| **Speed** | 0.08s | 5.78s | DP (72Ã— faster) |
| **Final Reward** | 3.50 | 3.50 | Tie |
| **Model Required** | âœ“ Yes | âœ— No | MC (model-free) |
| **Best For** | Known environments | Unknown environments | Context-dependent |

## ğŸ“ What's Here

```
.
â”œâ”€â”€ QUICKSTART.md                     # ğŸ‘ˆ Start here (2 min setup)
â”œâ”€â”€ README_GITHUB.md                  # Full documentation
â”œâ”€â”€ REPORT.pdf                        # Academic report (detailed)
â”œâ”€â”€ requirements.txt                  # Dependencies
â”œâ”€â”€ setup.py                          # Package installer
â”œâ”€â”€ .gitignore                        # Git configuration
â”œâ”€â”€ LICENSE                           # MIT License
â”œâ”€â”€ CONTRIBUTING.md                   # Contribution guide
â”‚
â”œâ”€â”€ task1_dynamic_programming/        # âœ“ Value Iteration
â”‚   â”œâ”€â”€ task1_main.py                # 3 experiments
â”‚   â”œâ”€â”€ dp_algorithms.py             # Core algorithm
â”‚   â””â”€â”€ environment_setup.py         # Maze environment
â”‚
â”œâ”€â”€ task2_monte_carlo/               # âœ“ First-Visit MC
â”‚   â”œâ”€â”€ task2_main.py                # 3 experiments
â”‚   â”œâ”€â”€ mc_algorithms.py             # Core algorithm
â”‚   â””â”€â”€ environment_setup.py         # Maze with viz
â”‚
â”œâ”€â”€ task3_analysis/                  # âœ“ Comparison
â”‚   â””â”€â”€ task3_main.py                # Decision framework
â”‚
â””â”€â”€ results/                         # Generated visualizations
    â”œâ”€â”€ task1_dp/      (10 plots)
    â”œâ”€â”€ task2_mc/      (3 plots)
    â””â”€â”€ task3_comparison/ (2 plots)
```

## ğŸ¯ What Each Task Does

### Task 1: Dynamic Programming
Tests **Value Iteration** with different hyperparameters:
- Tests Î³ âˆˆ {0.50, 0.70, 0.90, 0.99}
- Compares Policy Iteration vs Value Iteration
- Analyzes convergence curves

**Results**: âœ… 16 iterations, 0.08s, optimal Î³ = 0.99

### Task 2: Monte Carlo
Tests **First-Visit MC** with Îµ-greedy exploration:
- Tests Îµ âˆˆ {0.01, 0.05, 0.10, 0.30}
- Plots learning convergence over 5000 episodes
- Shows value distribution heatmaps

**Results**: âœ… 5000 episodes, 5.78s, optimal Îµ = 0.05

### Task 3: Comparative Analysis
Develops **decision framework** for algorithm selection:
- Compares 10 dimensions (speed, model req, scalability, etc.)
- Recommends algorithms for 8 scenarios
- Explains when to use each method

**Results**: âœ… Clear guidelines on DP vs MC trade-offs

## ğŸ”§ Installation

```bash
# Clone
git clone <repo>
cd reinforcement-learning-maze

# Setup
python -m venv venv
source venv/bin/activate  # or: venv\Scripts\activate (Windows)

# Install
pip install -r requirements.txt
```

## â–¶ï¸ Run Examples

```bash
# All tasks (recommended)
python task1_dynamic_programming/task1_main.py
python task2_monte_carlo/task2_main.py
python task3_analysis/task3_main.py

# Or individual tasks
cd task1_dynamic_programming && python task1_main.py
```

Output: Console metrics + PNG visualizations in `results/`

## ğŸ’¡ Key Insight

| | **DP** | **MC** |
|---|--------|--------|
| **Speed** | 0.08s | 5.78s |
| **Speedup** | **72Ã— faster** | baseline |
| **Model** | Needed | Not needed |
| **Best For** | Simulations | Real world |

## ğŸ“š Documentation

- **[QUICKSTART.md](QUICKSTART.md)** - 2 min setup guide
- **[README_GITHUB.md](README_GITHUB.md)** - Full docs
- **[CONTRIBUTING.md](CONTRIBUTING.md)** - How to extend
- **[REPORT.pdf](REPORT.pdf)** - Detailed methodology & theory

## ğŸš€ Next Steps

1. Run the examples above
2. Check `results/` for visualizations
3. Read `REPORT.pdf` for theory
4. Explore code in `task*/`
5. Try modifying hyperparameters

## ğŸ“‹ Requirements

- Python 3.8+
- NumPy, Matplotlib, SciPy (see requirements.txt)

## ğŸ“„ License

MIT - See [LICENSE](LICENSE)

## ğŸ¤ Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on extending this project.

---

**Ready to dive in?** â†’ [Start with QUICKSTART.md](QUICKSTART.md) âš¡

